{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDfYQD8eC6Ui"
   },
   "source": [
    "<h1>UCL School of Management</h1>\n",
    "<h2>MSIN0166 Data Engineering</h2>\n",
    "<h4>PySpark workshop - Theory & Practice</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDY5mUOLX6wU"
   },
   "source": [
    "<img src=\"https://ogirardot.files.wordpress.com/2015/05/future-of-spark.png\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmvlOQ6iFDQ1"
   },
   "source": [
    "# What is Spark?\n",
    "\n",
    " Apache Spark™ is a unified analytics engine for large-scale data processing. (Source: https://spark.apache.org)\n",
    "\n",
    "\n",
    "\n",
    "- For more details, go through Spark documentation: https://spark.apache.org/docs/latest/\n",
    "\n",
    "# Why use Spark ?\n",
    "\n",
    "- Speed : Spark framework can be up to 100 times faster than Hadoop when it comes to large scale data processing due to its in memory computing capability. \n",
    "\n",
    "- Ease of use: Spark's popularity led to significant open source community contributions. Databricks, its parent company, implemented a wide range of APIs for streaming solutions, machine learning or graph processing\n",
    "\n",
    "- Fault tolerance is guaranteed without data replication \n",
    "\n",
    "# Current Spark use cases\n",
    "\n",
    "- Netflix uses Spark for providing personalized trailers in real-time\n",
    "- Uber uses Spark for detecting driver abuse at global scale\n",
    "- Spotify uses Spark for creating your \"Wrapped 2019\" review.\n",
    "- Twitter uses Spark for identifying trending twitter hashtags. \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJd7xhVyFsFU"
   },
   "source": [
    "<h1>Spark architecture</h1>\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"></img>\n",
    "\n",
    "For a detailed explanation of how Spark runs, please visit: https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTPChqnyME8s"
   },
   "source": [
    "## Spark basic concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VctCDmmwOdoX"
   },
   "source": [
    "\n",
    "<img src=\"https://data-flair.training/blogs/wp-content/uploads/sites/2/2017/08/Internals-of-job-execution-in-spark.jpg\"></img>\n",
    "## Key elements\n",
    "- **Driver program**: This element runs the application\n",
    "- **SparkContext**: This is an object providing the application interface\n",
    "- **Cluster Manager**: Manages worker nodes\n",
    "- **Task**: An operation to be executed by the Spark application\n",
    "- **Job**: A collection of tasks\n",
    "- **Resilient Distributed Dataset**: more below\n",
    "- **Directed Acyclic Graph**: A sequence of tasks\n",
    "- **Worker node** : Executes tasks\n",
    "- **Cache**: Data storage used for future faster retrieval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Pk3QLigNl-S"
   },
   "source": [
    "<H3>Apache Spark libraries</h3>\n",
    "\n",
    "- **Spark Streaming**: Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams. (Source: https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"></img>\n",
    "*Spark Streaming receives real-time data from various sources (e.g. Kafka message queues), divides it into batches and sends it to the Spark Engine, which generates the results stream in batches*\n",
    "\n",
    "For more details, read the documentation: https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **Spark MLLib**: MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "    - ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "    - Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "    - Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "    - Persistence: saving and load algorithms, models, and Pipelines\n",
    "    - Utilities: linear algebra, statistics, data handling, etc.\n",
    "Source: https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "\n",
    "- **Spark GraphX** : GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/graphx-programming-guide.html\n",
    "\n",
    "- **Spark SQL**: Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine. It also provides powerful integration with the rest of the Spark ecosystem (e.g., integrating SQL query processing with machine learning).\n",
    "\n",
    "\n",
    "**Did you know?** \n",
    "\n",
    "Since its release, Apache Spark, the unified analytics engine, has seen rapid adoption by enterprises across a wide range of industries. Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. It has quickly become the largest open source community in big data, with over 1000 contributors from 250+ organizations.\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsT8v_2RSuWs"
   },
   "source": [
    "## Spark RDDs, DataSets and DataFrames\n",
    "\n",
    "\n",
    "- RDD\n",
    " \n",
    " - As data cannot fit into one single machine, it has to be distributed across multiple nodes for future processing.\n",
    " - Therefore, Spark partitions the data and distributes it across nodes. \n",
    "\n",
    "  - Spark brings the concept of a **resilient distributed dataset (RDD)**: a fault-tolerant collection of elements that can be operated on in parallel. \n",
    "\n",
    "    1. Resilient: An RDD has the ability to recover quickly, yet it comes with an additional requirement: **immutability**. When writing a Spark program, we create a directed acyclical graph consisting of a set of tasks, each generating an RDD after every \"transform\" operation (more on this below). \n",
    "\n",
    "    2. Distributed: The object is distributed across multiple nodes, dealing with fault tolerance.\n",
    "\n",
    "    3. Dataset: Set of data points.\n",
    "\n",
    "  - RDDs contain partitions of data and provide a programming interface\n",
    "\n",
    "  - There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "\n",
    "  - You could think of an RDD as being an immutable, partitioned collection of data points.\n",
    "\n",
    "\n",
    "\n",
    "  - RDD objects manage the interaction with distributeddata transparently\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **Datasets**: A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame, which is a Dataset of Row. A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **DataFrames**: A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n",
    "<br/>\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FthPI_47OGBr"
   },
   "source": [
    "## Spark actions and transformations\n",
    "\n",
    "- Actions:Calculate output values and trigger transformations\n",
    "\n",
    "Read the Spark documentation to see specific examples of Spark actions: https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "- Transformations: Create a new RDD with lazy execution\n",
    "\n",
    "Read the Spark documentation to see specific examples of Spark transformations: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AycPkkKxOGUL"
   },
   "source": [
    "## Spark caching & storage\n",
    "\n",
    "Spark is popular due to its fast, in-memory capabilities. These capabilities are achieved through persistence (or caching) among other optimizations.  \n",
    "\n",
    "Spark can cache a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). \n",
    "\n",
    "This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative\n",
    " algorithms and fast interactive use.\n",
    "\n",
    "Source: https://spark.apache.org\n",
    "\n",
    "**Spark storage levels**\n",
    "\n",
    "  - MEMORY_ONLY: store in memory, recompute when out of memory \n",
    "    - Typically the best option for repeated reading.\n",
    "  - MEMORY_AND_DISK: store in memory, save to disk when out of memory \n",
    "    - May be slower than recomputingunless computation is extensive\n",
    "  - MEMORY_AND_DISK_2: replicate each partition on 2 cluster node for fast failure recovery  \n",
    "  - DISK_ONLY:  Store the RDD partitions only on disk. \n",
    "\n",
    "\n",
    "**Persistent RDDs**\n",
    "\n",
    "For a better understanding of these concepts, read the documentation: https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#rdd-persistence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyJ6hHT3NmPn"
   },
   "source": [
    "## File systems\n",
    "\n",
    "**What is a file system**\n",
    "\n",
    "\n",
    "As volumes of data grow exponentially throughout the years, the open-source community and the large technology companies came up with data storage solutions in the form of distributed file systems for large, distributed, data-intensive applications.\n",
    "\n",
    "- **HDFS**: Hadoop Distributed File System: \n",
    "\n",
    "The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets.\n",
    "\n",
    "Source: https://hadoop.apache.org\n",
    "\n",
    "Other systems:\n",
    "- **GFS**: Google File System\n",
    "\n",
    "Read more about the Google File System in the original paper: https://research.google/pubs/pub51/\n",
    "\n",
    "\n",
    "- **EFS** : Amazon Elastic File System\n",
    "\n",
    "Read more about Amazon Elastic File System at: https://aws.amazon.com/efs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6COTmPlLNmgc"
   },
   "source": [
    "## MapReduce \n",
    "\n",
    "**MapReduce algorithm**: A distributed data processing algorithm, inspired by the functional programming paradigm and introduced by Google. \n",
    "\n",
    "MapReduce Algorithm uses the following three main steps:\n",
    "- A **Map** function: This takes a dataset (or a task) and divides it into smaller datasets (or subtasks). It then performs the required computation on each of the data samples (sub-tasks), in parallel. \n",
    "  - It performs the following two steps:\n",
    "    - Splits the input data into smaller sub-datasets\n",
    "    - Performs a mapping step on each sub-dataset.\n",
    "\n",
    "  -The output of this map function is a set of key-value pairs in the following form: <key,value> \n",
    "\n",
    "\n",
    "- A **Shuffle** function: This function takes a list of outputs from the map function and performs two steps:\n",
    "\n",
    "1. Merges all key-value pairs which have the same key, returning a <Key, List<Value>> pair.\n",
    "\n",
    "2. It sorts all key-value pairs by using keys, returning a <Key, List<Value>> output with sorted key-value pairs.\n",
    "\n",
    "\n",
    "- A **Reduce** function: It takes list of <Key, List<Value>> sorted pairs from the Shuffle function and performs an aggregation of values, based on their keys. \n",
    "\n",
    "\n",
    "\n",
    "**Hadoop MapReduce**\n",
    "\n",
    "Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.\n",
    "\n",
    "A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.\n",
    "\n",
    "Typically the compute nodes and the storage nodes are the same, that is, the MapReduce framework and the Hadoop Distributed File System (see HDFS Architecture Guide) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster.\n",
    "\n",
    "Source: https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTDTiT2_Nm2L"
   },
   "source": [
    "## Spark vs Hadoop\n",
    "\n",
    "- Data storage: in-memory vs disk\n",
    "- Fault tolerance: RDDs guarantee fault tolerance; \n",
    "- Hadoop uses replication to achieve fault tolerance\n",
    "- Spark is faster due to in-memory computation\n",
    "- Spark benefits of Linux, Windows and MacOS support\n",
    "- Spark can be used to process and modify real-time data; Hadoop Map-Reduce can process a batch of stored data\n",
    "\n",
    "<img src=\"https://data-flair.training/blogs/wp-content/uploads/sites/2/2016/09/Hadoop-MapReduce-vs-Apache-Spark.jpg\"></img>\n",
    "*Source: DataFlair*\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png\"></img>\n",
    "\n",
    "*Source: MongoDB*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJj5enyrPwax"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "Execute the cells below to:\n",
    "- Install PySpark\n",
    "- Mount Google Drive for data storage\n",
    "- Copy the San Francisco Bay Area Bike Share dataset inside your data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbgBh6SNNnpA"
   },
   "source": [
    "## Batch vs Stream processing examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59135,
     "status": "ok",
     "timestamp": 1579683283068,
     "user": {
      "displayName": "Teo Pop",
      "photoUrl": "",
      "userId": "04573610714739845033"
     },
     "user_tz": 0
    },
    "id": "_Bl7WeRsJAvd",
    "outputId": "e4c9cc3a-2315-44ea-8192-d24e654b0251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 62kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 44.6MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=ce4ae8f46eb66e642ff93b807f3034d3a3aef32c4af52d0470f7c449915c0a9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeZQkj4nJA_U"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "re8WyhGj7ADF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4XsoGNW7Iio"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58910,
     "status": "ok",
     "timestamp": 1579683373129,
     "user": {
      "displayName": "Teo Pop",
      "photoUrl": "",
      "userId": "04573610714739845033"
     },
     "user_tz": 0
    },
    "id": "b1hSpKizJLOl",
    "outputId": "14e11b46-58d4-43ce-9e07-1109396204b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n",
      "/content/drive/My Drive/Colab Notebooks\n",
      "/content/drive/My Drive/Colab Notebooks/data\n",
      "total 2021826\n",
      "-rw------- 1 root root       5647 Jan 22 08:56 station.csv\n",
      "-rw------- 1 root root 1989696383 Jan 22 08:56 status.csv\n",
      "-rw------- 1 root root   80208848 Jan 22 08:55 trip.csv\n",
      "-rw------- 1 root root     438063 Jan 22 08:55 weather.csv\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/\n",
    "!mkdir \"Colab Notebooks\"\n",
    "%cd \"Colab Notebooks\"\n",
    "!mkdir data\n",
    "%cd data\n",
    "!cp \"/content/drive/My Drive/Data-Engineering-Workshop-Week-2/data/sf-bay-bike-share-data/trip.csv\" .\n",
    "!cp \"/content/drive/My Drive/Data-Engineering-Workshop-Week-2/data/sf-bay-bike-share-data/weather.csv\" .\n",
    "!cp \"/content/drive/My Drive/Data-Engineering-Workshop-Week-2/data/sf-bay-bike-share-data/status.csv\" .\n",
    "!cp \"/content/drive/My Drive/Data-Engineering-Workshop-Week-2/data/sf-bay-bike-share-data/station.csv\" .\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHdiuKMSpWSa"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "station_rdd=sc.wholeTextFiles(\"station.csv\")\n",
    "station_df = sqlContext.read.csv(\"station.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "kAEicVuUMAMV",
    "outputId": "82c827c7-3eba-45cc-aa5b-4337e358fce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:06:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:07:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:08:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:09:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:10:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:11:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:12:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:13:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:15:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:16:02')]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df=sqlContext.read.csv(\"status.csv\",header=True)\n",
    "status_df.registerTempTable(\"status\")\n",
    "status_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciL1QRe_Q-3k"
   },
   "source": [
    "**Measuring popularity of each station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "75JHxA3IOfgE",
    "outputId": "09f2a272-bfab-4c68-b54b-66eb36f62e2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(station_id='10'), 1047141),\n",
       " (Row(station_id='16'), 1047141),\n",
       " (Row(station_id='32'), 872382),\n",
       " (Row(station_id='67'), 1047140),\n",
       " (Row(station_id='27'), 1047141),\n",
       " (Row(station_id='41'), 1047141),\n",
       " (Row(station_id='47'), 1047141),\n",
       " (Row(station_id='80'), 872134),\n",
       " (Row(station_id='3'), 1047113),\n",
       " (Row(station_id='21'), 1047141),\n",
       " (Row(station_id='23'), 1047141),\n",
       " (Row(station_id='24'), 1046949),\n",
       " (Row(station_id='38'), 1047141),\n",
       " (Row(station_id='4'), 1047100),\n",
       " (Row(station_id='63'), 1042279),\n",
       " (Row(station_id='65'), 1047141),\n",
       " (Row(station_id='29'), 1047141),\n",
       " (Row(station_id='46'), 1047141),\n",
       " (Row(station_id='58'), 1045067),\n",
       " (Row(station_id='61'), 1047141),\n",
       " (Row(station_id='76'), 1047140),\n",
       " (Row(station_id='14'), 1047141),\n",
       " (Row(station_id='22'), 1047141),\n",
       " (Row(station_id='42'), 1047141),\n",
       " (Row(station_id='56'), 1047141),\n",
       " (Row(station_id='64'), 1047141),\n",
       " (Row(station_id='68'), 1047140),\n",
       " (Row(station_id='6'), 1047142),\n",
       " (Row(station_id='7'), 1047142),\n",
       " (Row(station_id='34'), 1047141),\n",
       " (Row(station_id='35'), 1047137),\n",
       " (Row(station_id='49'), 1047141),\n",
       " (Row(station_id='54'), 1047141),\n",
       " (Row(station_id='5'), 1047142),\n",
       " (Row(station_id='26'), 1047141),\n",
       " (Row(station_id='31'), 872235),\n",
       " (Row(station_id='74'), 1047140),\n",
       " (Row(station_id='83'), 798868),\n",
       " (Row(station_id='37'), 1047141),\n",
       " (Row(station_id='70'), 1047140),\n",
       " (Row(station_id='11'), 1047141),\n",
       " (Row(station_id='25'), 1047141),\n",
       " (Row(station_id='45'), 1047141),\n",
       " (Row(station_id='51'), 1044272),\n",
       " (Row(station_id='59'), 1047141),\n",
       " (Row(station_id='75'), 1047140),\n",
       " (Row(station_id='77'), 1047139),\n",
       " (Row(station_id='8'), 1046570),\n",
       " (Row(station_id='12'), 1047141),\n",
       " (Row(station_id='36'), 1047141),\n",
       " (Row(station_id='84'), 731527),\n",
       " (Row(station_id='9'), 1047141),\n",
       " (Row(station_id='28'), 1047141),\n",
       " (Row(station_id='33'), 1047141),\n",
       " (Row(station_id='39'), 1046972),\n",
       " (Row(station_id='66'), 1047141),\n",
       " (Row(station_id='73'), 1040727),\n",
       " (Row(station_id='2'), 1046898),\n",
       " (Row(station_id='48'), 1047141),\n",
       " (Row(station_id='55'), 1046133),\n",
       " (Row(station_id='30'), 1047141),\n",
       " (Row(station_id='60'), 1047141),\n",
       " (Row(station_id='62'), 1045106),\n",
       " (Row(station_id='69'), 1047140),\n",
       " (Row(station_id='71'), 1047140),\n",
       " (Row(station_id='72'), 1047140),\n",
       " (Row(station_id='82'), 840950),\n",
       " (Row(station_id='13'), 1047141),\n",
       " (Row(station_id='50'), 1046973),\n",
       " (Row(station_id='57'), 1047141)]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_id=status_df.select('station_id')\n",
    "mapped_test = station_id.rdd.map(lambda item: (item,1))\n",
    "aggregated_station_counts = mapped_test.reduceByKey(lambda a,b:a+b)\n",
    "mapped_test.take(5)\n",
    "aggregated_station_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lA4E9kpynVjc"
   },
   "outputs": [],
   "source": [
    "def processing_function(filename,caching=True):\n",
    "  rdd_sample=sc.wholeTextFiles(filename)\n",
    "  rdd_map=rdd_sample.map(lambda item: (item, 1))\n",
    "  rdd_reduce=rdd_map.reduceByKey(lambda a,b: a+b)\n",
    "  if caching:\n",
    "    rdd_reduce.persist(StorageLevel.MEMORY_ONLY) # since we will read more than once, caching in Memory will make things quicker.\n",
    "  else:\n",
    "    rdd_reduce.persist(StorageLevel.DISK_ONLY) # Here is another option to control the caching behaviour\n",
    "  return rdd_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "De-Be1L7ZJc6"
   },
   "source": [
    "## Batch vs Stream processing time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qkVJLiRlpYlC",
    "outputId": "498d4d1b-7dbc-47d5-f1f4-cc4cdfa6336a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running item count on the station dataset, 3 trials - mean time with caching:  3.6523680686950684 , mean time without caching:  3.090451717376709\n"
     ]
    }
   ],
   "source": [
    "#Running an experiment to measure time spent processing\n",
    "from pyspark import StorageLevel\n",
    "from time import time\n",
    "\n",
    "resCaching = [] # for storing results\n",
    "resNoCache = []\n",
    "\n",
    "for i in range(3): # 3 samples \n",
    "    startTime = time() # start timer\n",
    "    processing_function(\"trip.csv\",caching=True).collect()\n",
    "    endTime = time()  \n",
    "    resCaching.append( endTime - startTime )\n",
    "    \n",
    "for i in range(3): # 3 samples\n",
    "    startTime = time()\n",
    "    processing_function(\"trip.csv\",caching=False).collect()\n",
    "    endTime = time()\n",
    "    resNoCache.append( endTime - startTime )\n",
    "\n",
    "meanTimeCaching = sum(resCaching)/len(resCaching)\n",
    "meanTimeNoCache = sum(resNoCache)/len(resNoCache)\n",
    "\n",
    "print('Running item count on the station dataset, 3 trials - mean time with caching: ', meanTimeCaching, ', mean time without caching: ', meanTimeNoCache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReQIjT86ZWzf"
   },
   "source": [
    "## Spark SQL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "yd6N9BfapZ5i",
    "outputId": "5e07a474-9105-4570-a5b3-e0ce5f8a3ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- dock_count: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- installation_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv(\"station.csv\",header=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "xITHD_enpaGP",
    "outputId": "95f22408-86a2-441b-8a8b-68e364905cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-------------------+----------+------------+-----------------+\n",
      "| id|                name|               lat|               long|dock_count|        city|installation_date|\n",
      "+---+--------------------+------------------+-------------------+----------+------------+-----------------+\n",
      "|  2|San Jose Diridon ...|         37.329732|-121.90178200000001|        27|    San Jose|         8/6/2013|\n",
      "|  3|San Jose Civic Ce...|         37.330698|        -121.888979|        15|    San Jose|         8/5/2013|\n",
      "|  4|Santa Clara at Al...|         37.333988|        -121.894902|        11|    San Jose|         8/6/2013|\n",
      "|  5|    Adobe on Almaden|         37.331415|          -121.8932|        19|    San Jose|         8/5/2013|\n",
      "|  6|    San Pedro Square|37.336721000000004|        -121.894074|        15|    San Jose|         8/7/2013|\n",
      "|  7|Paseo de San Antonio|         37.333798|-121.88694299999999|        15|    San Jose|         8/7/2013|\n",
      "|  8| San Salvador at 1st|         37.330165|-121.88583100000001|        15|    San Jose|         8/5/2013|\n",
      "|  9|           Japantown|         37.348742|-121.89471499999999|        15|    San Jose|         8/5/2013|\n",
      "| 10|  San Jose City Hall|         37.337391|        -121.886995|        15|    San Jose|         8/6/2013|\n",
      "| 11|         MLK Library|         37.335885|-121.88566000000002|        19|    San Jose|         8/6/2013|\n",
      "| 12|SJSU 4th at San C...|         37.332808|-121.88389099999999|        19|    San Jose|         8/7/2013|\n",
      "| 13|       St James Park|         37.339301|-121.88993700000002|        15|    San Jose|         8/6/2013|\n",
      "| 14|Arena Green / SAP...|         37.332692|        -121.900084|        19|    San Jose|         8/5/2013|\n",
      "| 16|SJSU - San Salvad...|37.333954999999996|        -121.877349|        15|    San Jose|         8/7/2013|\n",
      "| 21|   Franklin at Maple|         37.481758|        -122.226904|        15|Redwood City|        8/12/2013|\n",
      "| 22|Redwood City Calt...|37.486078000000006|-122.23208899999999|        25|Redwood City|        8/15/2013|\n",
      "| 23|San Mateo County ...|37.487615999999996|        -122.229951|        15|Redwood City|        8/15/2013|\n",
      "| 24|Redwood City Publ...|         37.484219|        -122.227424|        15|Redwood City|        8/12/2013|\n",
      "| 25|Stanford in Redwo...|          37.48537|-122.20328799999999|        15|Redwood City|        8/12/2013|\n",
      "| 26|Redwood City Medi...|         37.487682|        -122.223492|        15|Redwood City|        8/12/2013|\n",
      "+---+--------------------+------------------+-------------------+----------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "YWj2begOr4__",
    "outputId": "4b01358b-385a-40ad-a4c3-96e104704cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|San Jose Diridon ...|\n",
      "|San Jose Civic Ce...|\n",
      "|Santa Clara at Al...|\n",
      "|    Adobe on Almaden|\n",
      "|    San Pedro Square|\n",
      "|Paseo de San Antonio|\n",
      "| San Salvador at 1st|\n",
      "|           Japantown|\n",
      "|  San Jose City Hall|\n",
      "|         MLK Library|\n",
      "|SJSU 4th at San C...|\n",
      "|       St James Park|\n",
      "|Arena Green / SAP...|\n",
      "|SJSU - San Salvad...|\n",
      "|   Franklin at Maple|\n",
      "|Redwood City Calt...|\n",
      "|San Mateo County ...|\n",
      "|Redwood City Publ...|\n",
      "|Stanford in Redwo...|\n",
      "|Redwood City Medi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"station\")\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "dw9JOx1Xr9xp",
    "outputId": "9259f217-9c24-4714-8d6d-95cea8c5e606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "| id|                name|               lat|               long|dock_count|    city|installation_date|\n",
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "|  2|San Jose Diridon ...|         37.329732|-121.90178200000001|        27|San Jose|         8/6/2013|\n",
      "|  3|San Jose Civic Ce...|         37.330698|        -121.888979|        15|San Jose|         8/5/2013|\n",
      "|  4|Santa Clara at Al...|         37.333988|        -121.894902|        11|San Jose|         8/6/2013|\n",
      "|  5|    Adobe on Almaden|         37.331415|          -121.8932|        19|San Jose|         8/5/2013|\n",
      "|  6|    San Pedro Square|37.336721000000004|        -121.894074|        15|San Jose|         8/7/2013|\n",
      "|  7|Paseo de San Antonio|         37.333798|-121.88694299999999|        15|San Jose|         8/7/2013|\n",
      "|  8| San Salvador at 1st|         37.330165|-121.88583100000001|        15|San Jose|         8/5/2013|\n",
      "|  9|           Japantown|         37.348742|-121.89471499999999|        15|San Jose|         8/5/2013|\n",
      "| 10|  San Jose City Hall|         37.337391|        -121.886995|        15|San Jose|         8/6/2013|\n",
      "| 11|         MLK Library|         37.335885|-121.88566000000002|        19|San Jose|         8/6/2013|\n",
      "| 12|SJSU 4th at San C...|         37.332808|-121.88389099999999|        19|San Jose|         8/7/2013|\n",
      "| 13|       St James Park|         37.339301|-121.88993700000002|        15|San Jose|         8/6/2013|\n",
      "| 14|Arena Green / SAP...|         37.332692|        -121.900084|        19|San Jose|         8/5/2013|\n",
      "| 16|SJSU - San Salvad...|37.333954999999996|        -121.877349|        15|San Jose|         8/7/2013|\n",
      "| 80|Santa Clara Count...|         37.352601|-121.90573300000001|        15|San Jose|       12/31/2013|\n",
      "| 84|         Ryland Park|         37.342725|-121.89561699999999|        15|San Jose|         4/9/2014|\n",
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "san_jose= sqlContext.sql(\"SELECT * FROM station where city=='San Jose' LIMIT 100 \")\n",
    "san_jose.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7mSMfd6ZXK9"
   },
   "source": [
    "## Pandas vs Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHLKPFgipbDg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "start_time=time()\n",
    "pandas_df=pd.read_csv(\"status.csv\")\n",
    "end_time=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4d7AldxClDz"
   },
   "outputs": [],
   "source": [
    "start_time_spark=time()\n",
    "status_df = sqlContext.read.csv(\"status.csv\",header=True)\n",
    "end_time_spark=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wlMvU4NNCa8n",
    "outputId": "8c183ee2-ac73-47f8-a2dc-405b356c977e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent to load the Pandas DataFrame in memory: 66.91029930114746\n",
      "Time spent to load the Spark DataFrame in memory: 0.3091096878051758\n"
     ]
    }
   ],
   "source": [
    "print(\"Time spent to load the Pandas DataFrame in memory:\", end_time - start_time )\n",
    "print(\"Time spent to load the Spark DataFrame in memory:\", end_time_spark - start_time_spark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "WSxStaZnpbQp",
    "outputId": "8b774d31-dd8f-4d9b-8fe7-c00783e08c94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>bikes_available</th>\n",
       "      <th>docks_available</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2013/08/29 12:06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2013/08/29 12:07:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2013/08/29 12:08:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2013/08/29 12:09:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2013/08/29 12:10:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  bikes_available  docks_available                 time\n",
       "0           2                2               25  2013/08/29 12:06:01\n",
       "1           2                2               25  2013/08/29 12:07:01\n",
       "2           2                2               25  2013/08/29 12:08:01\n",
       "3           2                2               25  2013/08/29 12:09:01\n",
       "4           2                2               25  2013/08/29 12:10:01"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wvnKVLnsCDMX",
    "outputId": "1c9cdd23-6af5-4c55-ce8c-550cffd42d42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71984434, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "Flubdu40EuG3",
    "outputId": "59541b0b-49b9-4155-b497-c4699e3758ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|              2|             25|2013/08/29 12:06:01|\n",
      "|         2|              2|             25|2013/08/29 12:07:01|\n",
      "|         2|              2|             25|2013/08/29 12:08:01|\n",
      "|         2|              2|             25|2013/08/29 12:09:01|\n",
      "|         2|              2|             25|2013/08/29 12:10:01|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "eFqrajikFcGX",
    "outputId": "166e652f-0a0f-4ce7-bbc4-b8a9b1940e79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:06:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:07:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:08:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:09:01'),\n",
       " Row(station_id='2', bikes_available='2', docks_available='25', time='2013/08/29 12:10:01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0A3cJA83FmSJ",
    "outputId": "96ea8ac8-4d6c-47b0-a6f7-f7acd2781155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71984434"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azq61mbaZnNH"
   },
   "source": [
    "##  Inspect data\n",
    "Other functions that you can use to inspect your data are take() or takeSample(), but also countByKey(), countByValue() or collectAsMap()\n",
    "\n",
    "Add a few more commands from \n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "cDOhisbbsnbJ",
    "outputId": "9fb9e006-b181-4936-cdb0-beb24f40e50a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='2', name='San Jose Diridon Caltrain Station', lat='37.329732', long='-121.90178200000001', dock_count='27', city='San Jose', installation_date='8/6/2013'),\n",
       " Row(id='3', name='San Jose Civic Center', lat='37.330698', long='-121.888979', dock_count='15', city='San Jose', installation_date='8/5/2013'),\n",
       " Row(id='4', name='Santa Clara at Almaden', lat='37.333988', long='-121.894902', dock_count='11', city='San Jose', installation_date='8/6/2013'),\n",
       " Row(id='5', name='Adobe on Almaden', lat='37.331415', long='-121.8932', dock_count='19', city='San Jose', installation_date='8/5/2013'),\n",
       " Row(id='6', name='San Pedro Square', lat='37.336721000000004', long='-121.894074', dock_count='15', city='San Jose', installation_date='8/7/2013'),\n",
       " Row(id='7', name='Paseo de San Antonio', lat='37.333798', long='-121.88694299999999', dock_count='15', city='San Jose', installation_date='8/7/2013'),\n",
       " Row(id='8', name='San Salvador at 1st', lat='37.330165', long='-121.88583100000001', dock_count='15', city='San Jose', installation_date='8/5/2013'),\n",
       " Row(id='9', name='Japantown', lat='37.348742', long='-121.89471499999999', dock_count='15', city='San Jose', installation_date='8/5/2013'),\n",
       " Row(id='10', name='San Jose City Hall', lat='37.337391', long='-121.886995', dock_count='15', city='San Jose', installation_date='8/6/2013'),\n",
       " Row(id='11', name='MLK Library', lat='37.335885', long='-121.88566000000002', dock_count='19', city='San Jose', installation_date='8/6/2013')]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_jose.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "l1B_U57ksnqf",
    "outputId": "fc28a430-613f-40f3-96b8-de9b25daf507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/content/drive/My Drive/Colab Notebooks/data/station.csv',\n",
       "  'id,name,lat,long,dock_count,city,installation_date\\n2,San Jose Diridon Caltrain Station,37.329732,-121.90178200000001,27,San Jose,8/6/2013\\n3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013\\n4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013\\n5,Adobe on Almaden,37.331415,-121.8932,19,San Jose,8/5/2013\\n6,San Pedro Square,37.336721000000004,-121.894074,15,San Jose,8/7/2013\\n7,Paseo de San Antonio,37.333798,-121.88694299999999,15,San Jose,8/7/2013\\n8,San Salvador at 1st,37.330165,-121.88583100000001,15,San Jose,8/5/2013\\n9,Japantown,37.348742,-121.89471499999999,15,San Jose,8/5/2013\\n10,San Jose City Hall,37.337391,-121.886995,15,San Jose,8/6/2013\\n11,MLK Library,37.335885,-121.88566000000002,19,San Jose,8/6/2013\\n12,SJSU 4th at San Carlos,37.332808,-121.88389099999999,19,San Jose,8/7/2013\\n13,St James Park,37.339301,-121.88993700000002,15,San Jose,8/6/2013\\n14,Arena Green / SAP Center,37.332692,-121.900084,19,San Jose,8/5/2013\\n16,SJSU - San Salvador at 9th,37.333954999999996,-121.877349,15,San Jose,8/7/2013\\n21,Franklin at Maple,37.481758,-122.226904,15,Redwood City,8/12/2013\\n22,Redwood City Caltrain Station,37.486078000000006,-122.23208899999999,25,Redwood City,8/15/2013\\n23,San Mateo County Center,37.487615999999996,-122.229951,15,Redwood City,8/15/2013\\n24,Redwood City Public Library,37.484219,-122.227424,15,Redwood City,8/12/2013\\n25,Stanford in Redwood City,37.48537,-122.20328799999999,15,Redwood City,8/12/2013\\n26,Redwood City Medical Center,37.487682,-122.223492,15,Redwood City,8/12/2013\\n27,Mountain View City Hall,37.389218,-122.081896,15,Mountain View,8/16/2013\\n28,Mountain View Caltrain Station,37.394358000000004,-122.07671299999998,23,Mountain View,8/15/2013\\n29,San Antonio Caltrain Station,37.406940000000006,-122.10675800000001,23,Mountain View,8/15/2013\\n30,Evelyn Park and Ride,37.390277000000005,-122.066553,15,Mountain View,8/16/2013\\n31,San Antonio Shopping Center,37.400443,-122.10833799999999,15,Mountain View,12/31/2013\\n32,Castro Street and El Camino Real,37.385956,-122.083678,11,Mountain View,12/31/2013\\n33,Rengstorff Avenue / California Street,37.400240999999994,-122.099076,15,Mountain View,8/16/2013\\n34,Palo Alto Caltrain Station,37.443988,-122.164759,23,Palo Alto,8/14/2013\\n35,University and Emerson,37.444521,-122.16309299999999,11,Palo Alto,8/15/2013\\n36,California Ave Caltrain Station,37.429082,-122.14280500000001,15,Palo Alto,8/14/2013\\n37,Cowper at University,37.448598,-122.159504,11,Palo Alto,8/14/2013\\n38,Park at Olive,37.425683899999996,-122.13777749999998,15,Palo Alto,8/14/2013\\n41,Clay at Battery,37.795001,-122.39997,15,San Francisco,8/19/2013\\n42,Davis at Jackson,37.79728,-122.398436,15,San Francisco,8/19/2013\\n45,Commercial at Montgomery,37.794230999999996,-122.402923,15,San Francisco,8/19/2013\\n46,Washington at Kearney,37.795425,-122.40476699999999,15,San Francisco,8/19/2013\\n47,Post at Kearney,37.788975,-122.403452,19,San Francisco,8/19/2013\\n48,Embarcadero at Vallejo,37.799953,-122.398525,15,San Francisco,8/19/2013\\n49,Spear at Folsom,37.790302000000004,-122.39063700000001,19,San Francisco,8/20/2013\\n50,Harry Bridges Plaza (Ferry Building),37.795392,-122.394203,23,San Francisco,8/20/2013\\n51,Embarcadero at Folsom,37.791464000000005,-122.391034,19,San Francisco,8/20/2013\\n39,Powell Street BART,37.783871000000005,-122.408433,19,San Francisco,8/25/2013\\n54,Embarcadero at Bryant,37.787152,-122.38801299999999,15,San Francisco,8/20/2013\\n55,Temporary Transbay Terminal (Howard at Beale),37.789756,-122.39464299999999,23,San Francisco,8/20/2013\\n56,Beale at Market,37.792251,-122.39708600000002,19,San Francisco,8/20/2013\\n57,5th at Howard,37.781752000000004,-122.40512700000001,15,San Francisco,8/21/2013\\n58,San Francisco City Hall,37.77865,-122.41823500000001,19,San Francisco,8/21/2013\\n59,Golden Gate at Polk,37.781332,-122.418603,23,San Francisco,8/21/2013\\n60,Embarcadero at Sansome,37.80477,-122.40323400000001,15,San Francisco,8/21/2013\\n61,2nd at Townsend,37.780526,-122.39028799999998,27,San Francisco,8/22/2013\\n62,2nd at Folsom,37.785299,-122.39623600000002,19,San Francisco,8/22/2013\\n63,Howard at 2nd,37.786978000000005,-122.39810800000001,19,San Francisco,8/22/2013\\n64,2nd at South Park,37.782259,-122.392738,15,San Francisco,8/22/2013\\n65,Townsend at 7th,37.771058000000004,-122.402717,15,San Francisco,8/22/2013\\n66,South Van Ness at Market,37.774814,-122.418954,19,San Francisco,8/23/2013\\n67,Market at 10th,37.776619000000004,-122.41738500000001,27,San Francisco,8/23/2013\\n68,Yerba Buena Center of the Arts (3rd @ Howard),37.784878000000006,-122.40101399999999,19,San Francisco,8/23/2013\\n69,San Francisco Caltrain 2 (330 Townsend),37.7766,-122.39546999999999,23,San Francisco,8/23/2013\\n70,San Francisco Caltrain (Townsend at 4th),37.776617,-122.39526000000001,19,San Francisco,8/23/2013\\n71,Powell at Post (Union Square),37.788446,-122.408499,19,San Francisco,8/23/2013\\n72,Civic Center BART (7th at Market),37.781039,-122.411748,23,San Francisco,8/23/2013\\n73,Grant Avenue at Columbus Avenue,37.798522,-122.40724499999999,15,San Francisco,8/21/2013\\n74,Steuart at Market,37.794139,-122.394434,23,San Francisco,8/25/2013\\n75,Mechanics Plaza (Market at Battery),37.7913,-122.399051,19,San Francisco,8/25/2013\\n76,Market at 4th,37.786305,-122.40496599999999,19,San Francisco,8/25/2013\\n77,Market at Sansome,37.789625,-122.400811,27,San Francisco,8/25/2013\\n80,Santa Clara County Civic Center,37.352601,-121.90573300000001,15,San Jose,12/31/2013\\n82,Broadway St at Battery St,37.798541,-122.40086200000002,15,San Francisco,1/22/2014\\n83,Mezes Park,37.491269,-122.23623400000001,15,Redwood City,2/20/2014\\n84,Ryland Park,37.342725,-121.89561699999999,15,San Jose,4/9/2014\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return a fixed-size sampled subset of this RDD (currently requires numpy).\n",
    "station_rdd.takeSample(False,20,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E9nwZVRjsn26",
    "outputId": "49694cfa-c30d-4e5e-bdf8-8a1cb290aaa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByKey:  Count the number of elements for each key, and return the result to the master as a dictionary.\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1),('c',4)])\n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "efByS5N8t6JJ",
    "outputId": "310467e6-97dc-47e0-a75b-44b3abfbeb12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByValue:  Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "sorted(sc.parallelize([1, 2, 1, 2, 2,3]).countByValue().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrMmnw9pZvDo"
   },
   "source": [
    "## Creating new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "C4deyf2-t6Zq",
    "outputId": "cf77602e-e1e0-497c-ec93-4fd67715e087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+\n",
      "|station_id|bikes_available|docks_available|               time|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "|         2|              2|             25|2013/08/29 12:06:01|\n",
      "|         2|              2|             25|2013/08/29 12:07:01|\n",
      "|         2|              2|             25|2013/08/29 12:08:01|\n",
      "|         2|              2|             25|2013/08/29 12:09:01|\n",
      "|         2|              2|             25|2013/08/29 12:10:01|\n",
      "+----------+---------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRXBbi_2TA3a"
   },
   "source": [
    "The San Francisco district is cutting budgets for the Bike Sharing programme. It has decided to take out one bicycle at every station. Let's see how many bicycles are available tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "Y-LPPGL6pb8m",
    "outputId": "1ba0b1d4-052c-47cc-af61-78f387a29d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+------------------------+\n",
      "|station_id|bikes_available|docks_available|               time|bikes_available_tomorrow|\n",
      "+----------+---------------+---------------+-------------------+------------------------+\n",
      "|         2|              2|             25|2013/08/29 12:06:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:07:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:08:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:09:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:10:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:11:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:12:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:13:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:15:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:16:02|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:18:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:19:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:20:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:21:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:22:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:23:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:25:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:26:01|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:27:04|                     1.0|\n",
      "|         2|              2|             25|2013/08/29 12:29:01|                     1.0|\n",
      "+----------+---------------+---------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_df.withColumn('bikes_available_tomorrow', status_df.bikes_available -1 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jOMTFfFTmqG"
   },
   "source": [
    "Documentation available at: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWQcdPWqdZuO"
   },
   "source": [
    "## Exercise 1\n",
    "<p>Create a Spark RDD based on the status.csv file. Follow the examples above and extract 10 rows from the dataset</p>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VH_VqGb4c3rv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRRFWjMsc_-B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohxV79ovdAaT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDFHVCBuex_4"
   },
   "source": [
    "## Exercise 2\n",
    "<p>Practice the MapReduce algorithm and measure the popularity of each station by counting the number of station occurences.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKU63ZtpdAmN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHUIzSckdAxM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaucn8QcdA8Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJdtG8SJe6-5"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Use Spark SQL to create a Spark Dataframe from the station.csv file.\n",
    "\n",
    "Select all stations from the dataframe where the dock count is lower than 20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bofIvxGqe744"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgU7DbPNe8LC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJ6DSmr5e8nm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCR6fhUHfeI6"
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "4.1 Create a new column to show the bike share trip ride, expressed in minutes. \n",
    "\n",
    "**Hint**: Use the (.withColumn example above) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gIv6az2ht_S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOGg7mfjhuN-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAs8z663hx48"
   },
   "source": [
    "4.2 Finally, return all records in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjIiAEDNhuaj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4RHJHsekThS"
   },
   "source": [
    "# Exercise 5\n",
    "Count the number of occurences for each city in the station dataset by applying the MapReduce algorithm, similar to the Word Count example in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xud38rA3O7UP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PySpark_workshop_practice_Week_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
